ğŸ¯ Ã‰tape 1 â€“ config.py : pourquoi, comment, rÃ´le dans un pipeline ?
ğŸ”µ Pourquoi un fichier config.py ?

Dans un vrai pipeline, on manipule beaucoup de chemins, paramÃ¨tres, options, noms de fichiers, etc.

Sans fichier de config :

tu dupliques les chemins partout

si tu changes lâ€™arborescence â†’ tu dois tout modifier

ton orchestrateur devient un gros â€œspaghettiâ€

Avec config.py :

tous les chemins sont dans un seul endroit

on sÃ©pare logique & configuration

modularitÃ© â†’ on peut rÃ©utiliser les fonctions sans tout casser

clartÃ© â†’ chaque module lit dans config

Câ€™est un vrai rÃ©flexe de data engineer â­

ğŸ”µ OÃ¹ se place config.py ?

Tu lâ€™as mis dans :

notebooks/pipeline/config.py

ğŸ”µ Contenu conceptuel du config.py

config.py doit dÃ©finir :

1ï¸âƒ£ Les chemins principaux du projet (relatifs Ã  notebooks/)

chemin vers le rÃ©pertoire input (tous les JSON)

chemin vers le rÃ©pertoire done

chemin vers le rÃ©pertoire error

chemin vers les statiques (customers + refunds)

chemin vers les outputs CSV

2ï¸âƒ£ Les patterns des fichiers

prÃ©fixe des fichiers commandes : "orders_"

extension : .json

3ï¸âƒ£ Les conventions de date

format attendu dans les fichiers : %Y-%m-%d

format pour le CSV en sortie : %Y%m%d

4ï¸âƒ£ Optionnel : paramÃ¨tres de pipeline

activer logs dÃ©taillÃ©s ?

nombre de partitions Spark ?

encoding CSV ?

Pour lâ€™instant tu nâ€™en as pas besoin, mais la place est lÃ  si un jour tu veux.

---

Ã‰tape 2 = poser proprement la crÃ©ation de la SparkSession dans spark_session.py.

1ï¸âƒ£ RÃ´le de spark_session.py dans ton pipeline

En Spark, tout passe par la SparkSession :

câ€™est elle qui lit les fichiers (CSV, JSON, Parquetâ€¦),

qui applique les transformations,

qui lance les jobs visibles dans le Spark UI,

et qui gÃ¨re la config (nombre de partitions, logs, etc.).

Bon rÃ©flexe data ingÃ© :
ğŸ‘‰ une seule fonction qui crÃ©e cette session, dans un module dÃ©diÃ©
ğŸ‘‰ tous les autres modules lâ€™utilisent (orchestrator, io_readers, tests dans notebook)

Ã‡a Ã©vite :

dâ€™avoir des SparkSession.builder... copiÃ©s-collÃ©s partout,

dâ€™avoir des configs diffÃ©rentes suivant les scripts,

dâ€™oublier un paramÃ¨tre important Ã  un endroit.

2ï¸âƒ£ Local vs cluster dans ton contexte

Dans ton docker-compose, tu as :

un conteneur spark qui joue le rÃ´le de Spark Master (UI sur 8080)

un conteneur jupyter avec pyspark-notebook oÃ¹ tu codes.

Mais tu nâ€™as pas de Spark Worker dÃ©clarÃ© dans le docker-compose.yml.

Donc deux options thÃ©oriques :

Local mode (ce que tu fais aujourdâ€™hui)

SparkSession.builder.getOrCreate() sans .master(...)

Spark tourne â€œen localâ€ dans le conteneur Jupyter.

Tu auras une Spark UI sur le port 4040 de ce conteneur (si tu le mappes un jour).

Cluster mode (spark://spark:7077)`

il faudrait ajouter au moins un Worker dans ton docker-compose.

et configurer .master("spark://spark:7077").

Comme tu ne veux pas partir en usine Ã  gaz, on reste en local mode, ce qui est parfait pour :

apprendre les transformations Spark,

avoir un code simple,

et plus tard tu pourras brancher sur un cluster en changeant juste une ligne ici.

3ï¸âƒ£ Ce quâ€™on veut exactement dans spark_session.py

Objectif :

1 module : notebooks/pipeline/spark_session.py

1 fonction publique : create_spark_session(app_name: str = "FreshKartDailyPipeline")

centraliser la crÃ©ation de la session

ajouter 1â€“2 petits rÃ©glages utiles (ex : progression dans la console)

Tu utiliseras ensuite cette fonction :

dans lâ€™orchestrateur,

dans tes notebooks (Ã  la place de celle de freshkart_io Ã  terme).

4ï¸âƒ£ Code

Remarques :

pas de .master(...) â†’ on reste en local mode pour lâ€™instant, simple et fiable ;

si plus tard tu veux tester le master standalone, tu pourras juste ajouter :

.master("spark://spark:7077")

---

Ã©tape 3 ğŸ˜
Objectif : centraliser toute la lecture des donnÃ©es dans io_readers.py.

ğŸ§  Rappel pÃ©dagogique : rÃ´le de io_readers.py

Dans ton pipeline :

spark_session.py â†’ crÃ©e la SparkSession

config.py â†’ sait oÃ¹ sont les fichiers

io_readers.py â†’ sait comment les lire avec Spark

Pourquoi câ€™est utile :

tu sÃ©pares la configuration (chemins) de la logique de lecture ;

tous les autres modules (transformations, orchestrator, tests) appellent les mÃªmes fonctions pour lire les donnÃ©es ;

si tu changes un jour le format (CSV â†’ Parquet, autre cheminâ€¦), tu modifies un seul fichier.

Dans ton cas, io_readers.py va :

lire les fichiers statiques :

customers.csv

refunds.csv

lire un fichier JSON de commandes pour une date donnÃ©e :

orders_YYYY-MM-DD.json dans data/input

On prÃ©pare aussi dÃ¨s maintenant la gestion des erreurs de type â€œfichier manquantâ€, pour que lâ€™orchestrateur puisse dÃ©cider de mettre le fichier en error/.

---

ğŸš€ Ã‰tape suivante : transformations.py

Objectif pÃ©dagogique de cette Ã©tape :

comprendre comment Spark traite les DataFrames comme des tables distribuÃ©es

apprendre Ã  appliquer des rÃ¨gles mÃ©tier de maniÃ¨re fonctionnelle

manipuler les colonnes, filtrer, exploser, joindre, nettoyer

et surtout dÃ©couvrir comment Spark gÃ©nÃ¨re des plans de calcul (visible dans Spark UI)

ğŸ¯ Dans cette Ã©tape, on va coder 4 transformations :
1ï¸âƒ£ Filtrer les commandes payÃ©es
payment_status = 'paid'

2ï¸âƒ£ Joindre les clients et exclure is_active = false

â†’ On garde seulement les commandes de clients actifs

3ï¸âƒ£ Exploser les items

orders_df contient :

items: array<struct<qty, sku, unit_price>>


On doit passer de :

{
  order_id: 123,
  items: [
    {"qty": 1, "unit_price": 10},
    {"qty": 2, "unit_price": 5}
  ]
}


Ã€ :

(order_id, qty=1, unit_price=10)
(order_id, qty=2, unit_price=5)

4ï¸âƒ£ Filtrer prix nÃ©gatifs + renvoyer un DF des lignes rejetÃ©es

Les rÃ¨gles mÃ©tier disent :

si unit_price < 0 â†’ rejeter la ligne

garder trace des rejets

---

ğŸ¯ Objectif de cette Ã©tape (aggregations.py)

Ã€ partir de :

clean_items_df (items propres aprÃ¨s toutes les transformations),

refunds_df (remboursements bruts),

on veut produire un DataFrame avec les colonnes :

date

city

channel

orders_count

unique_customers

items_sold

gross_revenue_eur

refunds_eur

net_revenue_eur

Le tout agrÃ©gÃ© par : date Ã— city Ã— channel.

ğŸ§  Logique mÃ©tier (en langage humain)

On part de items_df (sortie de filter_negative_prices) qui contient :

order_id, customer_id, channel, created_at, city, sku, qty, unit_price.

On ajoute :

une colonne order_date = to_date(created_at)

une colonne line_revenue_eur = qty * unit_price

On regroupe par commande pour avoir des mÃ©triques par order :

items_sold = somme des qty

gross_revenue_eur = somme des line_revenue_eur

on garde customer_id, city, channel, order_date

CÃ´tÃ© refunds_df, on agrÃ¨ge les remboursements par order_id :

refunds_eur = somme des montants, en nÃ©gatif

On joint les deux sur order_id, on remplace les refunds manquants par 0.0.

Enfin, on regroupe par order_date, city, channel pour avoir :

orders_count = countDistinct(order_id)

unique_customers = countDistinct(customer_id)

items_sold = somme des items_sold par commande

gross_revenue_eur = somme

refunds_eur = somme

net_revenue_eur = gross + refunds

Et on renomme order_date â†’ date.

---

ğŸ¯ Ã‰tape suivante â€” Construire lâ€™ORCHESTRATEUR : plan dÃ©taillÃ©

Avant dâ€™Ã©crire la moindre ligne de code, on va dÃ©finir la structure exacte de orchestrator.py.

Tu dois me dire â€œOKâ€ avant quâ€™on code la premiÃ¨re brique.

ğŸ§± Structure finale du futur orchestrator.py

Voici ce que tu vas trouver dedans :

1ï¸âƒ£ Import des modules internes

create_spark_session

read_customers / read_refunds / read_orders_for_date

transformations (4 fonctions)

compute_daily_city_sales

write_daily_summary_csv

file_management (prochain module)

2ï¸âƒ£ Fonction utilitaire : rÃ©cupÃ©rer la liste des dates Ã  traiter

Option simple :

soit tu passes une liste de dates en paramÃ¨tre

soit tu listes tous les fichiers JSON du dossier input

on extrait la date depuis le nom :
orders_2025-03-01.json â†’ 2025-03-01

Comme tu veux un pipeline qui rattrape le retard, câ€™est parfait.

3ï¸âƒ£ La fonction principale : run_pipeline_for_dates(dates: list[str])

Pour chaque date :

lire le fichier JSON

appliquer transformations

nettoyer les prix nÃ©gatifs

calculer les agrÃ©gations

Ã©crire le CSV

dÃ©placer le fichier dans done/

ou dans error/ si Ã§a plante

4ï¸âƒ£ Fonction â€œrun()â€ globale

Un truc du genre :

def run():
    spark = create_spark_session()
    customers_df = read_customers(spark)
    refunds_df = read_refunds(spark)
    dates = list_available_dates()
    run_pipeline_for_dates(spark, customers_df, refunds_df, dates)


Puis tu appelles :

if __name__ == "__main__":
    run()