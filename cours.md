ğŸ¯ Ã‰tape 1 â€“ config.py : pourquoi, comment, rÃ´le dans un pipeline ?
ğŸ”µ Pourquoi un fichier config.py ?

Dans un vrai pipeline, on manipule beaucoup de chemins, paramÃ¨tres, options, noms de fichiers, etc.

Sans fichier de config :

tu dupliques les chemins partout

si tu changes lâ€™arborescence â†’ tu dois tout modifier

ton orchestrateur devient un gros â€œspaghettiâ€

Avec config.py :

tous les chemins sont dans un seul endroit

on sÃ©pare logique & configuration

modularitÃ© â†’ on peut rÃ©utiliser les fonctions sans tout casser

clartÃ© â†’ chaque module lit dans config

Câ€™est un vrai rÃ©flexe de data engineer â­

ğŸ”µ OÃ¹ se place config.py ?

Tu lâ€™as mis dans :

notebooks/pipeline/config.py

ğŸ”µ Contenu conceptuel du config.py

config.py doit dÃ©finir :

1ï¸âƒ£ Les chemins principaux du projet (relatifs Ã  notebooks/)

chemin vers le rÃ©pertoire input (tous les JSON)

chemin vers le rÃ©pertoire done

chemin vers le rÃ©pertoire error

chemin vers les statiques (customers + refunds)

chemin vers les outputs CSV

2ï¸âƒ£ Les patterns des fichiers

prÃ©fixe des fichiers commandes : "orders_"

extension : .json

3ï¸âƒ£ Les conventions de date

format attendu dans les fichiers : %Y-%m-%d

format pour le CSV en sortie : %Y%m%d

4ï¸âƒ£ Optionnel : paramÃ¨tres de pipeline

activer logs dÃ©taillÃ©s ?

nombre de partitions Spark ?

encoding CSV ?

Pour lâ€™instant tu nâ€™en as pas besoin, mais la place est lÃ  si un jour tu veux.

---

Ã‰tape 2 = poser proprement la crÃ©ation de la SparkSession dans spark_session.py.

1ï¸âƒ£ RÃ´le de spark_session.py dans ton pipeline

En Spark, tout passe par la SparkSession :

câ€™est elle qui lit les fichiers (CSV, JSON, Parquetâ€¦),

qui applique les transformations,

qui lance les jobs visibles dans le Spark UI,

et qui gÃ¨re la config (nombre de partitions, logs, etc.).

Bon rÃ©flexe data ingÃ© :
ğŸ‘‰ une seule fonction qui crÃ©e cette session, dans un module dÃ©diÃ©
ğŸ‘‰ tous les autres modules lâ€™utilisent (orchestrator, io_readers, tests dans notebook)

Ã‡a Ã©vite :

dâ€™avoir des SparkSession.builder... copiÃ©s-collÃ©s partout,

dâ€™avoir des configs diffÃ©rentes suivant les scripts,

dâ€™oublier un paramÃ¨tre important Ã  un endroit.

2ï¸âƒ£ Local vs cluster dans ton contexte

Dans ton docker-compose, tu as :

un conteneur spark qui joue le rÃ´le de Spark Master (UI sur 8080)

un conteneur jupyter avec pyspark-notebook oÃ¹ tu codes.

Mais tu nâ€™as pas de Spark Worker dÃ©clarÃ© dans le docker-compose.yml.

Donc deux options thÃ©oriques :

Local mode (ce que tu fais aujourdâ€™hui)

SparkSession.builder.getOrCreate() sans .master(...)

Spark tourne â€œen localâ€ dans le conteneur Jupyter.

Tu auras une Spark UI sur le port 4040 de ce conteneur (si tu le mappes un jour).

Cluster mode (spark://spark:7077)`

il faudrait ajouter au moins un Worker dans ton docker-compose.

et configurer .master("spark://spark:7077").

Comme tu ne veux pas partir en usine Ã  gaz, on reste en local mode, ce qui est parfait pour :

apprendre les transformations Spark,

avoir un code simple,

et plus tard tu pourras brancher sur un cluster en changeant juste une ligne ici.

3ï¸âƒ£ Ce quâ€™on veut exactement dans spark_session.py

Objectif :

1 module : notebooks/pipeline/spark_session.py

1 fonction publique : create_spark_session(app_name: str = "FreshKartDailyPipeline")

centraliser la crÃ©ation de la session

ajouter 1â€“2 petits rÃ©glages utiles (ex : progression dans la console)

Tu utiliseras ensuite cette fonction :

dans lâ€™orchestrateur,

dans tes notebooks (Ã  la place de celle de freshkart_io Ã  terme).

4ï¸âƒ£ Code

Remarques :

pas de .master(...) â†’ on reste en local mode pour lâ€™instant, simple et fiable ;

si plus tard tu veux tester le master standalone, tu pourras juste ajouter :

.master("spark://spark:7077")

---

Ã©tape 3 ğŸ˜
Objectif : centraliser toute la lecture des donnÃ©es dans io_readers.py.

ğŸ§  Rappel pÃ©dagogique : rÃ´le de io_readers.py

Dans ton pipeline :

spark_session.py â†’ crÃ©e la SparkSession

config.py â†’ sait oÃ¹ sont les fichiers

io_readers.py â†’ sait comment les lire avec Spark

Pourquoi câ€™est utile :

tu sÃ©pares la configuration (chemins) de la logique de lecture ;

tous les autres modules (transformations, orchestrator, tests) appellent les mÃªmes fonctions pour lire les donnÃ©es ;

si tu changes un jour le format (CSV â†’ Parquet, autre cheminâ€¦), tu modifies un seul fichier.

Dans ton cas, io_readers.py va :

lire les fichiers statiques :

customers.csv

refunds.csv

lire un fichier JSON de commandes pour une date donnÃ©e :

orders_YYYY-MM-DD.json dans data/input

On prÃ©pare aussi dÃ¨s maintenant la gestion des erreurs de type â€œfichier manquantâ€, pour que lâ€™orchestrateur puisse dÃ©cider de mettre le fichier en error/.

---

ğŸš€ Ã‰tape suivante : transformations.py

Objectif pÃ©dagogique de cette Ã©tape :

comprendre comment Spark traite les DataFrames comme des tables distribuÃ©es

apprendre Ã  appliquer des rÃ¨gles mÃ©tier de maniÃ¨re fonctionnelle

manipuler les colonnes, filtrer, exploser, joindre, nettoyer

et surtout dÃ©couvrir comment Spark gÃ©nÃ¨re des plans de calcul (visible dans Spark UI)

ğŸ¯ Dans cette Ã©tape, on va coder 4 transformations :
1ï¸âƒ£ Filtrer les commandes payÃ©es
payment_status = 'paid'

2ï¸âƒ£ Joindre les clients et exclure is_active = false

â†’ On garde seulement les commandes de clients actifs

3ï¸âƒ£ Exploser les items

orders_df contient :

items: array<struct<qty, sku, unit_price>>


On doit passer de :

{
  order_id: 123,
  items: [
    {"qty": 1, "unit_price": 10},
    {"qty": 2, "unit_price": 5}
  ]
}


Ã€ :

(order_id, qty=1, unit_price=10)
(order_id, qty=2, unit_price=5)

4ï¸âƒ£ Filtrer prix nÃ©gatifs + renvoyer un DF des lignes rejetÃ©es

Les rÃ¨gles mÃ©tier disent :

si unit_price < 0 â†’ rejeter la ligne

garder trace des rejets